{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
      "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2024.6.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets rouge transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 KB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 KB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 KB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
      "Installing collected packages: regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.9.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (2.1.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BART model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only 10% of the training and test data\n",
    "train_val_size = int(len(dataset['train']) * 0.1)\n",
    "test_size = int(len(dataset['test']) * 0.1)\n",
    "\n",
    "train_val_data = dataset['train'].select(range(train_val_size))\n",
    "test_data = dataset['test'].select(range(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually split train and validation\n",
    "val_size = int(train_val_size * 0.1)\n",
    "train_size = train_val_size - val_size\n",
    "\n",
    "train_data = train_val_data.select(range(train_size))\n",
    "val_data = train_val_data.select(range(train_size, train_val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 1024\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        inputs = self.tokenizer(item['article'], max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(item['highlights'], max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': labels['input_ids'].squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = SummarizationDataset(train_data, tokenizer, MAX_LENGTH)\n",
    "val_dataset = SummarizationDataset(val_data, tokenizer, MAX_LENGTH)\n",
    "test_dataset = SummarizationDataset(test_data, tokenizer, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with early stopping\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "N_EPOCHS = 3\n",
    "PATIENCE = 2\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, device)\n",
    "    valid_loss = evaluate(model, val_loader, device)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'bart_summarizer.pt')\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "    \n",
    "    if epochs_without_improvement == PATIENCE:\n",
    "        print(f'Early stopping after {epoch+1} epochs without improvement.')\n",
    "        break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('bart_summarizer.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary\n",
    "def generate_summary(model, tokenizer, article, max_length=150):\n",
    "    inputs = tokenizer(article, max_length=1024, truncation=True, return_tensors='pt').to(device)\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=max_length, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████| 6460/6460 [33:21<00:00,  3.23it/s]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████| 718/718 [01:16<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.093\n",
      "\t Val. Loss: 0.077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████| 6460/6460 [33:14<00:00,  3.24it/s]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████| 718/718 [01:14<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02\n",
      "\tTrain Loss: 0.060\n",
      "\t Val. Loss: 0.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█████▌                                                   | 624/6460 [03:13<29:41,  3.28it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training: 100%|████████████████████████████████████████████████████████| 6460/6460 [33:19<00:00,  3.23it/s]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████| 718/718 [01:16<00:00,  9.39it/s]\n",
      "/tmp/ipykernel_1967/2875787693.py:144: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('bart_summarizer.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03\n",
      "\tTrain Loss: 0.046\n",
      "\t Val. Loss: 0.083\n",
      "Early stopping after 3 epochs without improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████████████████████████████████████| 288/288 [08:35<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 F1 Score: 0.30388266783945317\n",
      "ROUGE-2 F1 Score: 0.11718122237973874\n",
      "ROUGE-L F1 Score: 0.28229055415944043\n",
      "\n",
      "Sample Article:\n",
      "(CNN)Lady Antebellum singer Hillary Scott's tour bus caught fire on a Texas freeway Thursday morning, but everyone on board was safely evacuated. Michael Barnett captured dramatic video of the fire, on Interstate 30 just northeast of Dallas, and uploaded it to CNN iReport. Smoke and flames poured from the rear of the bus as traffic slowed to a crawl and Barnett slowly approached in his vehicle. As he drew closer to the bus, Barnett decided to stop filming because he didn't know what to expect. \"...\n",
      "\n",
      "Actual Summary:\n",
      "Country band Lady Antebellum's bus caught fire Thursday on a Texas freeway.\n",
      "A CNN iReporter captured the dramatic scene on video.\n",
      "Singer Hillary Scott shared a pic of the charred bus on Instagram.\n",
      "\n",
      "Generated Summary:\n",
      "Hillary Scott's tour bus caught fire on a Texas freeway Thursday morning.\n",
      "Michael Barnett captured dramatic video of the fire, on Interstate 30 just northeast of Dallas.\n",
      "\"It was shocking. I didn't know what I was about to see,\" Barnett says.\n",
      "Scott is set to perform at the Academy of Country Music Awards on Sunday.\n",
      "bart_summarizer.pt\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics\n",
    "rouge = Rouge()\n",
    "\n",
    "def calculate_rouge(hypotheses, references):\n",
    "    return rouge.get_scores(hypotheses, references, avg=True)\n",
    "\n",
    "# Test the model\n",
    "test_articles = []\n",
    "test_summaries = []\n",
    "generated_summaries = []\n",
    "\n",
    "for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "    articles = tokenizer.batch_decode(batch['input_ids'], skip_special_tokens=True)\n",
    "    summaries = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "    \n",
    "    for article in articles:\n",
    "        generated = generate_summary(model, tokenizer, article)\n",
    "        \n",
    "        test_articles.append(article)\n",
    "        test_summaries.append(summaries[articles.index(article)])\n",
    "        generated_summaries.append(generated)\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "rouge_scores = calculate_rouge(generated_summaries, test_summaries)\n",
    "\n",
    "print(\"ROUGE-1 F1 Score:\", rouge_scores['rouge-1']['f'])\n",
    "print(\"ROUGE-2 F1 Score:\", rouge_scores['rouge-2']['f'])\n",
    "print(\"ROUGE-L F1 Score:\", rouge_scores['rouge-l']['f'])\n",
    "\n",
    "# Print a sample summary\n",
    "sample_idx = random.randint(0, len(test_articles) - 1)\n",
    "print(\"\\nSample Article:\")\n",
    "print(test_articles[sample_idx][:500] + \"...\")  \n",
    "print(\"\\nActual Summary:\")\n",
    "print(test_summaries[sample_idx])\n",
    "print(\"\\nGenerated Summary:\")\n",
    "print(generated_summaries[sample_idx])\n",
    "\n",
    "# Created/Modified files during execution:\n",
    "print(\"bart_summarizer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMPLE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "U8K-3VaWCO-u"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 4.433\n",
      "Epoch: 02, Train Loss: 3.720\n",
      "Epoch: 03, Train Loss: 3.211\n",
      "Epoch: 04, Train Loss: 2.941\n",
      "Epoch: 05, Train Loss: 2.592\n",
      "Epoch: 06, Train Loss: 2.249\n",
      "Epoch: 07, Train Loss: 2.055\n",
      "Epoch: 08, Train Loss: 1.701\n",
      "Epoch: 09, Train Loss: 1.443\n",
      "Epoch: 10, Train Loss: 1.232\n",
      "Epoch: 11, Train Loss: 1.061\n",
      "Epoch: 12, Train Loss: 0.907\n",
      "Epoch: 13, Train Loss: 0.767\n",
      "Epoch: 14, Train Loss: 0.634\n",
      "Epoch: 15, Train Loss: 0.507\n",
      "Epoch: 16, Train Loss: 0.536\n",
      "Epoch: 17, Train Loss: 0.371\n",
      "Epoch: 18, Train Loss: 0.311\n",
      "Epoch: 19, Train Loss: 0.280\n",
      "Epoch: 20, Train Loss: 0.242\n",
      "Epoch: 21, Train Loss: 0.178\n",
      "Epoch: 22, Train Loss: 0.155\n",
      "Epoch: 23, Train Loss: 0.137\n",
      "Epoch: 24, Train Loss: 0.107\n",
      "Epoch: 25, Train Loss: 0.096\n",
      "Epoch: 26, Train Loss: 0.091\n",
      "Epoch: 27, Train Loss: 0.069\n",
      "Epoch: 28, Train Loss: 0.056\n",
      "Epoch: 29, Train Loss: 0.052\n",
      "Epoch: 30, Train Loss: 0.042\n",
      "Epoch: 31, Train Loss: 0.041\n",
      "Epoch: 32, Train Loss: 0.037\n",
      "Epoch: 33, Train Loss: 0.032\n",
      "Epoch: 34, Train Loss: 0.032\n",
      "Epoch: 35, Train Loss: 0.027\n",
      "Epoch: 36, Train Loss: 0.031\n",
      "Epoch: 37, Train Loss: 0.021\n",
      "Epoch: 38, Train Loss: 0.020\n",
      "Epoch: 39, Train Loss: 0.019\n",
      "Epoch: 40, Train Loss: 0.018\n",
      "Epoch: 41, Train Loss: 0.016\n",
      "Epoch: 42, Train Loss: 0.015\n",
      "Epoch: 43, Train Loss: 0.015\n",
      "Epoch: 44, Train Loss: 0.014\n",
      "Epoch: 45, Train Loss: 0.013\n",
      "Epoch: 46, Train Loss: 0.013\n",
      "Epoch: 47, Train Loss: 0.012\n",
      "Epoch: 48, Train Loss: 0.012\n",
      "Epoch: 49, Train Loss: 0.011\n",
      "Epoch: 50, Train Loss: 0.010\n",
      "Epoch: 51, Train Loss: 0.012\n",
      "Epoch: 52, Train Loss: 0.010\n",
      "Epoch: 53, Train Loss: 0.010\n",
      "Epoch: 54, Train Loss: 0.009\n",
      "Epoch: 55, Train Loss: 0.009\n",
      "Epoch: 56, Train Loss: 0.009\n",
      "Epoch: 57, Train Loss: 0.009\n",
      "Epoch: 58, Train Loss: 0.010\n",
      "Epoch: 59, Train Loss: 0.008\n",
      "Epoch: 60, Train Loss: 0.008\n",
      "Epoch: 61, Train Loss: 0.008\n",
      "Epoch: 62, Train Loss: 0.007\n",
      "Epoch: 63, Train Loss: 0.007\n",
      "Epoch: 64, Train Loss: 0.007\n",
      "Epoch: 65, Train Loss: 0.007\n",
      "Epoch: 66, Train Loss: 0.007\n",
      "Epoch: 67, Train Loss: 0.006\n",
      "Epoch: 68, Train Loss: 0.006\n",
      "Epoch: 69, Train Loss: 0.006\n",
      "Epoch: 70, Train Loss: 0.006\n",
      "Epoch: 71, Train Loss: 0.006\n",
      "Epoch: 72, Train Loss: 0.007\n",
      "Epoch: 73, Train Loss: 0.005\n",
      "Epoch: 74, Train Loss: 0.006\n",
      "Epoch: 75, Train Loss: 0.006\n",
      "Epoch: 76, Train Loss: 0.006\n",
      "Epoch: 77, Train Loss: 0.006\n",
      "Epoch: 78, Train Loss: 0.005\n",
      "Epoch: 79, Train Loss: 0.005\n",
      "Epoch: 80, Train Loss: 0.005\n",
      "Epoch: 81, Train Loss: 0.005\n",
      "Epoch: 82, Train Loss: 0.005\n",
      "Epoch: 83, Train Loss: 0.005\n",
      "Epoch: 84, Train Loss: 0.005\n",
      "Epoch: 85, Train Loss: 0.005\n",
      "Epoch: 86, Train Loss: 0.004\n",
      "Epoch: 87, Train Loss: 0.005\n",
      "Epoch: 88, Train Loss: 0.004\n",
      "Epoch: 89, Train Loss: 0.004\n",
      "Epoch: 90, Train Loss: 0.004\n",
      "Epoch: 91, Train Loss: 0.004\n",
      "Epoch: 92, Train Loss: 0.004\n",
      "Epoch: 93, Train Loss: 0.004\n",
      "Epoch: 94, Train Loss: 0.004\n",
      "Epoch: 95, Train Loss: 0.004\n",
      "Epoch: 96, Train Loss: 0.004\n",
      "Epoch: 97, Train Loss: 0.004\n",
      "Epoch: 98, Train Loss: 0.004\n",
      "Epoch: 99, Train Loss: 0.003\n",
      "Epoch: 100, Train Loss: 0.004\n",
      "\n",
      "Article: The quick brown fox jumps over the lazy dog. It was a beautiful day in the forest. The birds were singing and the trees were swaying in the breeze.\n",
      "Actual Summary: Fox jumps over dog on a nice day in the forest.\n",
      "Generated Summary: fox jumps over dog on a nice day in the forest .\n",
      "\n",
      "Article: Scientists have discovered a new species of dinosaur in Argentina. The fossils suggest it was one of the largest animals to ever walk the Earth.\n",
      "Actual Summary: New giant dinosaur species found in Argentina.\n",
      "Generated Summary: new giant dinosaur species found in argentina .\n",
      "\n",
      "Article: A new study shows that drinking coffee may have health benefits. Researchers found that moderate coffee consumption is associated with a lower risk of heart disease.\n",
      "Actual Summary: Coffee drinking linked to lower heart disease risk.\n",
      "Generated Summary: coffee drinking linked to lower heart disease risk .\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Sample dataset\n",
    "sample_data = [\n",
    "    {\n",
    "        \"article\": \"The quick brown fox jumps over the lazy dog. It was a beautiful day in the forest. The birds were singing and the trees were swaying in the breeze.\",\n",
    "        \"summary\": \"Fox jumps over dog on a nice day in the forest.\"\n",
    "    },\n",
    "    {\n",
    "        \"article\": \"Scientists have discovered a new species of dinosaur in Argentina. The fossils suggest it was one of the largest animals to ever walk the Earth.\",\n",
    "        \"summary\": \"New giant dinosaur species found in Argentina.\"\n",
    "    },\n",
    "    {\n",
    "        \"article\": \"A new study shows that drinking coffee may have health benefits. Researchers found that moderate coffee consumption is associated with a lower risk of heart disease.\",\n",
    "        \"summary\": \"Coffee drinking linked to lower heart disease risk.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Tokenizer\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "# Vocabulary building\n",
    "def build_vocab(data, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for item in data:\n",
    "        counter.update(tokenize(item['article']))\n",
    "        counter.update(tokenize(item['summary']))\n",
    "\n",
    "    vocab = {'<unk>': 0, '<pad>': 1, '<sos>': 2, '<eos>': 3}\n",
    "    for word, count in counter.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_vocab(sample_data)\n",
    "\n",
    "# Constants\n",
    "MAX_LENGTH = 100\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Dataset class\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, data, vocab, max_length):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        article = tokenize(item['article'])[:self.max_length]\n",
    "        summary = tokenize(item['summary'])[:self.max_length]\n",
    "\n",
    "        article_ids = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in article] + [self.vocab['<eos>']]\n",
    "        summary_ids = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in summary] + [self.vocab['<eos>']]\n",
    "\n",
    "        # Pad sequences to ensure equal length in each batch\n",
    "        article_ids = article_ids + [self.vocab['<pad>']] * (self.max_length - len(article_ids))\n",
    "        summary_ids = summary_ids + [self.vocab['<pad>']] * (self.max_length - len(summary_ids))\n",
    "\n",
    "\n",
    "        return torch.tensor(article_ids), torch.tensor(summary_ids)\n",
    "\n",
    "# Create datasets\n",
    "dataset = SummarizationDataset(sample_data, vocab, MAX_LENGTH)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Transformer model\n",
    "class TransformerSummarizer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
    "        super(TransformerSummarizer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        trg = self.embedding(trg) * math.sqrt(self.d_model)\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        trg = self.pos_encoder(trg)\n",
    "\n",
    "        src_mask = self.transformer.generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg.size(1)).to(trg.device)\n",
    "\n",
    "        output = self.transformer(src.transpose(0, 1), trg.transpose(0, 1), src_mask, trg_mask)\n",
    "        output = self.fc_out(output.transpose(0, 1))\n",
    "\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = len(vocab)\n",
    "D_MODEL = 128\n",
    "NHEAD = 4\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "DIM_FEEDFORWARD = 256\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransformerSummarizer(VOCAB_SIZE, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD, DROPOUT).to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n",
    "\n",
    "# Training function\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        src, trg = batch\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:, :-1])\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Training loop\n",
    "N_EPOCHS = 100\n",
    "CLIP = 1\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}')\n",
    "\n",
    "# Generate summary\n",
    "def generate_summary(model, src, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    src_tensor = src.unsqueeze(0).to(device)\n",
    "    trg_tensor = torch.tensor([[vocab['<sos>']]], dtype=torch.long).to(device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output = model(src_tensor, trg_tensor)\n",
    "\n",
    "        pred_token = output.argmax(2)[:, -1].item()\n",
    "        trg_tensor = torch.cat([trg_tensor, torch.tensor([[pred_token]], dtype=torch.long).to(device)], dim=1)\n",
    "\n",
    "        if pred_token == vocab['<eos>']:\n",
    "            break\n",
    "\n",
    "    return trg_tensor.squeeze(0)\n",
    "\n",
    "# Test the model\n",
    "for item in sample_data:\n",
    "    article = item['article']\n",
    "    actual_summary = item['summary']\n",
    "\n",
    "    src_tensor = torch.tensor([vocab.get(token, vocab['<unk>']) for token in tokenize(article)], dtype=torch.long).to(device)\n",
    "    generated_ids = generate_summary(model, src_tensor)\n",
    "    generated_summary = ' '.join([list(vocab.keys())[list(vocab.values()).index(idx)] for idx in generated_ids if idx not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]])\n",
    "\n",
    "    print(\"\\nArticle:\", article)\n",
    "    print(\"Actual Summary:\", actual_summary)\n",
    "    print(\"Generated Summary:\", generated_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
